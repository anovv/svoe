{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>A set of low-code, scalable and highly configurable pipelines for financial data research, streaming and batch feature engineering, predictive model training, real-time inference and backtesting. Built on top of Ray, the framework allows to build and scale your custom pipelines from multi-core laptop to a cluster of 1000s of nodes.</p>"},{"location":"#why-use-svoe","title":"Why use Svoe?","text":""},{"location":"#services","title":"Services","text":"<ul> <li><code>svoe featurizer</code> - Featurizer.</li> <li><code>svoe trainer</code> - Trainer.</li> <li><code>svoe backtester</code> - Backtester.</li> <li><code>svoe -h</code> - Print help message and exit.</li> </ul>"},{"location":"architecture-overview/","title":"Architecture Overview","text":""},{"location":"backtester-overview/","title":"Backtester","text":""},{"location":"backtester-overview/#overview","title":"Overview","text":""},{"location":"backtester-overview/#key-features","title":"Key features","text":"<ul> <li>Event-driven:</li> <li>Integrated with Featurizer: Use the same config and feature definitions to automatically construct streaming version of your features and use them for real-time model inference</li> <li>Real-time model inference: Automatically plug models from Trainer into your strategy to get real-time low latency inference/predictions</li> <li>Distributed:</li> </ul>"},{"location":"featurizer-data-ingest/","title":"Data Ingest Pipeline","text":"<p>Featurizer provides a scalable, configurable and extensible data ingest pipeline which takes raw user data and puts it in Featurizer storage. It takes care of such things as indexing, compaction, per-data type resource allocation and many other data engineering related problems. It integrates with DataSourceDefinition class so users can easily add their own processing logic in a modular way without spinning up and maintaining data engineering infrastructure.</p>"},{"location":"featurizer-data-ingest/#usage","title":"Usage","text":"<ul> <li>CLI: <code>svoe featurizer run-data-ingest &lt;path_to_config&gt;</code></li> <li>API: <code>FeaturizerDataIngestPipelineRunner.run(path_to_config)</code></li> </ul>"},{"location":"featurizer-data-ingest/#config","title":"Config","text":"<p>TODO describe options</p> <pre><code>provider_name: cryptotick\nbatch_size: 12\nmax_executing_tasks: 10\ndata_source_files:\n  - data_source_definition: featurizer.data_definitions.common.l2_book_incremental.cryptotick.cryptotick_l2_book_incremental.CryptotickL2BookIncrementalData\n    files_and_sizes:\n      - ['limitbook_full/20230201/BINANCE_SPOT_BTC_USDT.csv.gz', 252000]\n      - ['limitbook_full/20230202/BINANCE_SPOT_BTC_USDT.csv.gz', 252000]\n      - ['limitbook_full/20230203/BINANCE_SPOT_BTC_USDT.csv.gz', 252000]\n      - ['limitbook_full/20230204/BINANCE_SPOT_BTC_USDT.csv.gz', 252000]\n</code></pre>"},{"location":"featurizer-data-source-definitions/","title":"DataSourceDefinition and Data Ingestion pipeline","text":""},{"location":"featurizer-data-source-definitions/#datasourcedefinition-overview","title":"DataSourceDefinition overview","text":"<p>Similar to <code>FeatureDefinition</code>, <code>DataSourceDefinition</code> defines a blueprint for a data source - a sequence  of events which framework treats as an input to our feature calculation pipelines. <code>DataSourceDefinition</code> always plays a role of a terminal node (a leaf) in a <code>FeatureDefinition</code>/<code>Feature</code> tree. Unlike <code>FeatureDefinition</code>, <code>DataSourceDefinition</code>  does not have any dependencies and does not perform any feature calculations - the purpose of this class is to hold metadata about event schema, possible preprocessing steps and other info. Let's take a look at an example <code>DataSourceDefinition</code></p> <pre><code>class MyDataSourceDefinition(DataSourceDefinition):\n\n    @classmethod\n    def event_schema(cls) -&gt; EventSchema:\n        # This method defines schema of the event\n        raise NotImplemented\n\n    @classmethod\n    def preprocess_impl(cls, raw_df: DataFrame) -&gt; DataFrame:\n        # Any preprocessing logic to get a sequence of EventSchema-like\n        # records in a form of a DataFrame from raw data (raw_df)\n        raise NotImplemented\n</code></pre> <p>See CryptotickL2BookIncrementalData for an example of a data source representing  incremental order book updates from Cryptotick data provider.</p>"},{"location":"featurizer-feature-definition/","title":"Features and Feature Definitions","text":""},{"location":"featurizer-feature-definition/#overview","title":"Overview","text":"<p>Feature is defined as an independent time based process, a set of timestamped events with identical schemas and contextual meaning, which in the context of our framework are represented either as a real-time stream of events (in case of real-time processing) or as a sequence of recorded events, stored as a (possibly distributed) dataframe (in case of historical/batch processing)</p> <p>Feature Definition is an abstraction to define a blueprint for a time-series based feature in a modular way. In this contex, you can view a separate feature as a result of applying feature-sepicfic params to feature definition, i.e. <code>Feature = FeatureDefinition + params</code></p> <p>In code, these abstractions are represented as <code>Feature</code> and <code>FeatureDefinition</code> classes. Users are not supposed to  construct <code>Feature</code> objects directly and are expected to either use existing feature definitions or to implement their own by subclassing <code>FeatureDefinition</code>.</p>"},{"location":"featurizer-feature-definition/#featuredefinition-overview","title":"FeatureDefinition overview","text":"<p><code>FeatureDefinition</code>  is a base class for all custom feature definitions. To implement a new feature definition, user must subclass it and implement key methods. Here is an example</p> <pre><code>class MyFeatureDefinitionFD(FeatureDefinition):\n\n    @classmethod\n    def event_schema(cls) -&gt; EventSchema:\n        # This method defines schema of the event\n        raise NotImplemented\n\n    @classmethod\n    def stream(\n        cls,\n        dep_upstreams: Dict['Feature', Stream],\n        feature_params: Dict\n    ) -&gt; Union[Stream, Tuple[Stream, Any]]:\n        # Contains logic to compute events for this feature based on upstream\n        # dependencies and user-provided params.\n        # Uses Streamz library for declarative stream processing API\n        raise NotImplemented\n\n    @classmethod\n    def dep_upstream_schema(\n        cls, \n        dep_schema: str = Optional[None]\n    ) -&gt; List[Union[str, Type[DataDefinition]]]:\n        # Specifies upstream dependencies for this FeatureDefinition as a list\n        # of DataDefinition's\n        raise NotImplemented\n\n    @classmethod\n    def group_dep_ranges(\n        cls,\n        feature: Feature,\n        dep_ranges: Dict[Feature, List[BlockMeta]]\n    ) -&gt; IntervalDict:\n        # Logic to group dependant input data (dep_ranges) into atomic blocks \n        # for parallel bulk processing of each group. The most basic case is \n        # identity_grouping(...): newly produced data blocks depend only on \n        # the current block (i.e. simple map operation)\n        # For more complicated example, consider feature with 1 dependency, \n        # which produces window-aggregated calculations: here, for each new \n        # data block, we need to group all the dependant blocks which fall \n        # into that window (this is implemented in windowed_grouping(...) method)\n        # There are other more complicated cases, for example time buckets of \n        # fixed lengths (for OHLCV as an example), and custom groupings based\n        # on data blocks content (see L2SnapshotFD as an example)\n        raise NotImplemented\n\n</code></pre> <p>As can be seen from the example above, <code>FeatureDefintion</code> class describes a tree-like structure, where the root is the current <code>FeatureDefintion</code> and the leafs are <code>DataSourceDefinition</code> classes which produce all the dependent features. Similarly, when framework builds <code>Feature</code> objects, each object is a tree-like structure, uniquely identified by it's dependencies and parameters (see <code>Feature</code> class for more info).</p> <p>For more examples please see examples section</p>"},{"location":"featurizer-overview/","title":"Featurizer","text":""},{"location":"featurizer-overview/#overview","title":"Overview","text":"<p>Featurizer is a framework which combines feature storage, feature calculation engine and user facing SDKs and configs to define features for real-time and batch (historical) processing on time series data. It is build on top of Ray and leverages Ray's distributed memory to produce feature sets/feature-lable sets for ML training.</p>"},{"location":"featurizer-overview/#key-features","title":"Key features","text":"<ul> <li>Flexible computation model: define modular FeatureDefinition\u2019s to calculate features on historical data as well as live data streams using the same code</li> <li>High configurability: Define yaml configs to produce feature sets/feature-label sets for unsupervised/supervised learning and analysis</li> <li>Scalable execution: Built on top of Ray, Featurizer allows for horizontally scalable feature calculations on historical data</li> <li>Scalable data storage: Unified data model and data access API for time-series data sources and user defined features allows for easy data discovery and retrieval</li> <li>Zero-copy in-memory data access : Featurizer integrates with Ray\u2019s distributed in-memory storage, allowing to use Ray\u2019s distributed ML frameworks (XGBoost, PyTorch, RLLib) for predictive modelling without moving data to the third party storage</li> </ul>"},{"location":"featurizer-quick-start/","title":"Featurizer Quick Start","text":"<ul> <li>Pick existing or define your own FeatureDefinition (see more in Feature Definitions section)</li> <li>Create Featurizer Config<ul> <li>Define start and end dates (more in Data Model)</li> <li>Pick which features to store by setting to_store (more in Storage)</li> <li>Define label feature by setting lookahead_shift (more in Labeling)</li> <li>Define features in feature_configs. Each feature is a result of applying feature_params and data_params    to FeatureDefinition.</li> </ul> </li> </ul> <p>Example config:   <code>start_date: '2023-02-01 10:00:00'   end_date: '2023-02-01 11:00:00'   label_feature_index: 0   label_lookahead: '5s'   features_to_store: [0, 1]   feature_configs:     - feature_definition: price.mid_price_fd       name: mid_price       params:         data_source: &amp;id001           - exchange: BINANCE             instrument_type: spot             symbol: BTC-USDT         feature:           1:             dep_schema: cryptotick             sampling: 1s     - feature_definition: volatility.volatility_stddev_fd       data_params: *id001       feature_params:         2:           dep_schema: cryptotick           sampling: 1s</code> - Run Featurizer   * CLI: <code>svoe featurizer run &lt;path_to_config&gt;</code>   * Python API: <code>Featurizer.run(path='path_to_config')</code></p> <p>Featurizer will compile a graph of tasks, execute it in a distributed manner over the cluster and store   the resulted distributed dataframe (FeatureLabelSet) in cluster memory and optionally in persistent storage.   The above config will result in following dataframe: </p> <p>```           timestamp  receipt_timestamp  label_mid_price-mid_price  mid_price-mid_price  feature_VolatilityStddevFD_62271b09-volatility   0     1.675234e+09       1.675234e+09                  23084.800            23084.435                                        0.000547   1     1.675234e+09       1.675234e+09                  23083.760            23084.355                                        0.040003   2     1.675234e+09       1.675234e+09                  23083.505            23084.635                                        0.117757   3     1.675234e+09       1.675234e+09                  23084.610            23085.020                                        0.257091   4     1.675234e+09       1.675234e+09                  23084.725            23084.800                                        0.242034   ...            ...                ...                        ...                  ...                                             ...</p> <p>```</p>"},{"location":"featurizer-storage/","title":"Featurizer Storage","text":""},{"location":"featurizer-storage/#overview","title":"Overview","text":"<p>Featurizer stores contents of features and data sources in blocks of timestamp-sorted records. For range-based queries and  other parametrized data access and exploration it keeps an index of all the blocks metadata (i.e. start and end timestamps, in-memory and  on-disk size, user-defined parameters, etc.) in a SQL database (currently supports MySQL or SQLite). Each block is represented  as a pandas DataFrame when loaded in memory or as a gzip-compressed parquet file when stored on disk or blob storage.</p>"},{"location":"featurizer-storage/#data-models","title":"Data Models","text":"<p>There are 4 main user-facing data models</p> <ul> <li>Block</li> </ul> <p>Single block of data, currently a simple pandas dataframe</p> <pre><code>Block = pd.DataFrame\n</code></pre> <ul> <li>BlockRange</li> </ul> <p>Represents a range of consecutive timestamp-sorted blocks. These are treated as a single range that contains no gaps.  Blocks are considered consecutive if time difference between them is no more than user-defined delta.</p> <pre><code>BlockRange = List[Block]\n</code></pre> <ul> <li>BlockMeta Represents block metadata: feature/data source name, key or id, time range, size, etc.</li> </ul> <pre><code>BlockMeta = Dict\n</code></pre> <ul> <li>BlockRangeMeta Similarly to BlockRange, represents metadata for consecutive blocks.</li> </ul> <pre><code>BlockRangeMeta = List[BlockMeta]\n</code></pre>"},{"location":"featurizer-storage/#sql-tables","title":"SQL Tables","text":"<p>Metadata about features/data sources and feature/data source blocks is stored in 4 tables</p> <ul> <li><code>features_metadata</code></li> <li><code>data_sources_metadata</code></li> <li><code>feature_blocks_metadata</code></li> <li><code>data_source_blocks_metadata</code></li> </ul>"},{"location":"featurizer-storage/#data-store-adapters","title":"Data Store Adapters","text":"<p>Featurizer provides <code>DataStoreAdapter</code> class to implement custom read/write operations for block storage. Users are able to extend this class with their own logic.</p> <pre><code>class DataStoreAdapter:\n\n    def load_df(self, path: str, **kwargs) -&gt; pd.DataFrame:\n        raise NotImplementedError\n\n    def store_df(self, path: str, df: pd.DataFrame, **kwargs):\n        raise NotImplementedError\n\n    def make_feature_block_path(self, item: FeatureBlockMetadata) -&gt; str:\n        raise NotImplementedError\n\n    def make_data_source_block_path(self, item: DataSourceBlockMetadata) -&gt; str:\n        raise NotImplementedError\n</code></pre> <p>Featurizer includes implementation two data store adapters:</p> <ul> <li><code>LocalDataStoreAdapter</code> Stores and reads data from local filesystem</li> <li><code>RemoteDataStoreAdapter</code> Stores and reads data from S3</li> </ul> <p>By default, <code>LocalDataStoreAdapter</code> is used</p>"},{"location":"featurizer-storage/#data-exploration-api","title":"Data Exploration API","text":"<p>TODO implement data exploration api</p>"},{"location":"featurizer-streaming/","title":"Featurizer Real Time Streaming","text":""},{"location":"featurizer-synthetic-data-sources/","title":"Synthetic Data Sources","text":""},{"location":"featurizer-task-graph/","title":"Featurizer Task Graph","text":""},{"location":"featurizer-task-graph/#overview","title":"Overview","text":"<p>The core of Featurizer's offline feature calculation is in building and executing Task Graph.</p> <p>Task Graph is a graph of load, preprocess and user-defined feature specific calculation logic tasks automatically built by the framework.</p>"},{"location":"featurizer-task-graph/#how-building-featureset-graph-works","title":"How Building FeatureSet Graph Works","text":"<p>As mentioned in Features and Feature Definitions section, each <code>Feature</code> is a tree-like structure with <code>DataSource</code> leafs, dependent features as nodes and target feature as a root. When user defines all necessary features in a <code>FeaturizerConfig</code>, the framework builds all the feature trees and their interdependencies and then maps corresponding feature calculation tasks (which are defined by <code>FeatureDefinition:stream</code> method) and data source load tasks into a graph. The tasks are intorconnected by passing dataframes as inputs and returns which are stored in Ray's distributed memory</p> <p>TODO explain range blocks</p> <p>See more in <code>featurizer.task_graph.builder.py</code></p>"},{"location":"featurizer-task-graph/#label-featureset-featurelabelset-graph","title":"Label + FeatureSet = FeatureLabelSet Graph","text":"<p>If user </p>"},{"location":"featurizer-task-graph/#point-in-time-joins","title":"Point-in-time Joins","text":""},{"location":"installation/","title":"Installation","text":"<p><code>pip install svoe</code> - Install python package.</p> <p><code>svoe standalone</code> - Launch standalone setup on your laptop. This will start local Ray cluster, create and populate  SQLite database, spin up MLFlow tracking server and load sample data from remote store (S3).</p>"},{"location":"quick-start/","title":"Quick start","text":""},{"location":"trainer-overview/","title":"Trainer","text":""},{"location":"trainer-overview/#overview","title":"Overview","text":""},{"location":"trainer-overview/#key-features","title":"Key features","text":"<ul> <li>Highly configurable: Trainer provides unified API and configs to train and evaluate predictive models using various ML libraries (XGBoost, PyTorch, RLLib)</li> <li>Integrated with Featurizer: Use distributed in-memory FeatureLabelSet data structure to train your predictive models without extra data pipelines</li> <li>Scalability: Data-parallel distributed training for all supported frameworks</li> <li>Hyperparameter optimization: Use Ray Tune to optimize hyperparameters and pick the best model</li> <li>Model and metadata storage: Trainer provides easy API for model access and metadata discovery by integrating with MLFlow</li> </ul>"}]}